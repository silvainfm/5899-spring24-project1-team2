{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('finished_annotation_5.2k.xlsx', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['title & content', 'summary', 'description', 'content', 'title', 'human_label_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Positive', 'Negative', 'Neutral', 'Neutral ', ' Neutral'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['human_label_sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['human_label_sentiment'] = data['human_label_sentiment'].apply(lambda x: 'Neutral' if x.strip() == 'Neutral' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Positive', 'Negative', 'Neutral'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['human_label_sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment = data[data['human_label_sentiment'] != 'No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative', 'Neutral'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment['human_label_sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_label_sentiment\n",
       "Negative    1801\n",
       "Neutral      948\n",
       "Positive     836\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment['human_label_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3585, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/wt3y86vj50sg2s6pkc5y62l00000gn/T/ipykernel_1719/1396676065.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sentiment['sentiment_score'] = data_sentiment['human_label_sentiment'].apply(lambda x: 0 if x == 'Neutral' else 1 if x == 'Positive' else -1)\n"
     ]
    }
   ],
   "source": [
    "data_sentiment['sentiment_score'] = data_sentiment['human_label_sentiment'].apply(lambda x: 0 if x == 'Neutral' else 1 if x == 'Positive' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment = data_sentiment[['title & content', 'human_label_sentiment', 'sentiment_score']]\n",
    "data_sentiment.columns = ['text', 'human_label', 'labels']\n",
    "data_sentiment.to_csv('prepared_sentiment_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>human_label</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shareholders v. Tesla, Nasdaq's diversity rule...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Robert Half Named One of Barron's Most Sustain...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The new electric USPS mail truck is America‚Äö...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FedEx closing more locations, planning to furl...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FedEx Parks Planes, Maersk Cancels Sails: Worl...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>American Airlines Pilots' Union Calls Strike A...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>Ford recalling over 1.2 MILLION cars over 'ser...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>Ford Making EVs Means Turning the Clock Back 1...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>Former SVB chief says the Fed never discussed ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>‚ÄòWe have money and power‚Äô: older Americans...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3585 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text human_label  labels\n",
       "3     Shareholders v. Tesla, Nasdaq's diversity rule...    Positive       1\n",
       "5     Robert Half Named One of Barron's Most Sustain...    Positive       1\n",
       "6     The new electric USPS mail truck is America‚Äö...    Positive       1\n",
       "8     FedEx closing more locations, planning to furl...    Negative      -1\n",
       "9     FedEx Parks Planes, Maersk Cancels Sails: Worl...    Negative      -1\n",
       "...                                                 ...         ...     ...\n",
       "5211  American Airlines Pilots' Union Calls Strike A...     Neutral       0\n",
       "5212  Ford recalling over 1.2 MILLION cars over 'ser...    Negative      -1\n",
       "5213  Ford Making EVs Means Turning the Clock Back 1...     Neutral       0\n",
       "5214  Former SVB chief says the Fed never discussed ...    Negative      -1\n",
       "5215  ‚ÄòWe have money and power‚Äô: older Americans...    Negative      -1\n",
       "\n",
       "[3585 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3585.000000\n",
       "mean     3379.271409\n",
       "std      1773.055330\n",
       "min       333.000000\n",
       "25%      2033.000000\n",
       "50%      3192.000000\n",
       "75%      4678.000000\n",
       "max      7436.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment['len'] = data_sentiment['text'].apply(lambda x: len(x))\n",
    "data_sentiment['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_sentiment[['text', 'human_label', 'labels']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'human_label', 'labels'],\n",
       "    num_rows: 3585\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'human_label', 'labels'],\n",
       "        num_rows: 2868\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'human_label', 'labels'],\n",
       "        num_rows: 717\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0878c256981c4c73ae4fef9c1654c974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e021fca7844048e0b8d57db7a664bf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT Model on all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 222818a5-d1da-4126-add9-8573dca2e940)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 47d62d3f-f8e2-4005-97df-2ef02a2892a6)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7992b227-e8c6-4faa-b620-c9418ed426ad)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\")  # mps is for Apple Silicon GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3729f39e9442e902f926082f37c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e3fda810ad489bbf7a3da861607350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "# Tokenize the text data in the train and test datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the compute_metrics function for accuracy\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c640024522b24982a4b5c208499ae289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3215, 'learning_rate': 5e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2506, 'learning_rate': 6.672443674176777e-06, 'epoch': 2.79}\n",
      "{'train_runtime': 3544.7119, 'train_samples_per_second': 2.427, 'train_steps_per_second': 0.304, 'train_loss': 0.2774076461791992, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d184e21a555c437e80328d7e761ebb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45036745071411133, 'eval_accuracy': 0.3905160390516039, 'eval_runtime': 29.7873, 'eval_samples_per_second': 24.071, 'eval_steps_per_second': 3.021, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the accuracy\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT Model on only last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109,484,547 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the BERT model\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,307 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83f6ccdc4df43aa9f6e57d300282fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1094, 'learning_rate': 5e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1153, 'learning_rate': 6.672443674176777e-06, 'epoch': 2.79}\n",
      "{'train_runtime': 443.4699, 'train_samples_per_second': 19.402, 'train_steps_per_second': 2.429, 'train_loss': 0.11023234878299186, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dec67c23d747d3a2e5f76b68599e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.473304808139801, 'eval_accuracy': 0.3877266387726639, 'eval_runtime': 28.7564, 'eval_samples_per_second': 24.934, 'eval_steps_per_second': 3.13, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the accuracy\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert_sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471e7b45e45f412286524525fced5042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93feaa0e1dec4c458b2231bdc205445c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/wt3y86vj50sg2s6pkc5y62l00000gn/T/ipykernel_1719/2250388178.py:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "# Tokenize the text data in the train and test datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the compute_metrics function for accuracy\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267128a5e0cc4db49dae91301c682413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1815, 'learning_rate': 5e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1668, 'learning_rate': 4.856733524355301e-05, 'epoch': 2.79}\n",
      "{'loss': 0.1674, 'learning_rate': 4.713467048710602e-05, 'epoch': 4.18}\n",
      "{'loss': 0.1689, 'learning_rate': 4.570200573065903e-05, 'epoch': 5.57}\n",
      "{'loss': 0.1638, 'learning_rate': 4.426934097421204e-05, 'epoch': 6.96}\n",
      "{'loss': 0.1673, 'learning_rate': 4.2836676217765046e-05, 'epoch': 8.36}\n",
      "{'loss': 0.1667, 'learning_rate': 4.140401146131805e-05, 'epoch': 9.75}\n",
      "{'loss': 0.1606, 'learning_rate': 3.997134670487106e-05, 'epoch': 11.14}\n",
      "{'loss': 0.167, 'learning_rate': 3.853868194842407e-05, 'epoch': 12.53}\n",
      "{'loss': 0.1625, 'learning_rate': 3.7106017191977077e-05, 'epoch': 13.93}\n",
      "{'loss': 0.163, 'learning_rate': 3.567335243553009e-05, 'epoch': 15.32}\n",
      "{'loss': 0.1752, 'learning_rate': 3.4240687679083095e-05, 'epoch': 16.71}\n",
      "{'loss': 0.1583, 'learning_rate': 3.280802292263611e-05, 'epoch': 18.11}\n",
      "{'loss': 0.166, 'learning_rate': 3.1375358166189114e-05, 'epoch': 19.5}\n",
      "{'loss': 0.1614, 'learning_rate': 2.9942693409742123e-05, 'epoch': 20.89}\n",
      "{'loss': 0.1672, 'learning_rate': 2.851002865329513e-05, 'epoch': 22.28}\n",
      "{'loss': 0.1523, 'learning_rate': 2.707736389684814e-05, 'epoch': 23.68}\n",
      "{'loss': 0.1723, 'learning_rate': 2.5644699140401145e-05, 'epoch': 25.07}\n",
      "{'loss': 0.1623, 'learning_rate': 2.4212034383954154e-05, 'epoch': 26.46}\n",
      "{'loss': 0.1629, 'learning_rate': 2.2779369627507164e-05, 'epoch': 27.86}\n",
      "{'loss': 0.1661, 'learning_rate': 2.1346704871060173e-05, 'epoch': 29.25}\n",
      "{'loss': 0.161, 'learning_rate': 1.991404011461318e-05, 'epoch': 30.64}\n",
      "{'loss': 0.1651, 'learning_rate': 1.8481375358166188e-05, 'epoch': 32.03}\n",
      "{'loss': 0.1667, 'learning_rate': 1.7048710601719198e-05, 'epoch': 33.43}\n",
      "{'loss': 0.1573, 'learning_rate': 1.5616045845272207e-05, 'epoch': 34.82}\n",
      "{'loss': 0.1636, 'learning_rate': 1.4183381088825215e-05, 'epoch': 36.21}\n",
      "{'loss': 0.1697, 'learning_rate': 1.2750716332378224e-05, 'epoch': 37.6}\n",
      "{'loss': 0.1543, 'learning_rate': 1.1318051575931232e-05, 'epoch': 39.0}\n",
      "{'loss': 0.1602, 'learning_rate': 9.885386819484241e-06, 'epoch': 40.39}\n",
      "{'loss': 0.1651, 'learning_rate': 8.452722063037249e-06, 'epoch': 41.78}\n",
      "{'loss': 0.1609, 'learning_rate': 7.020057306590258e-06, 'epoch': 43.18}\n",
      "{'loss': 0.1592, 'learning_rate': 5.587392550143267e-06, 'epoch': 44.57}\n",
      "{'loss': 0.1707, 'learning_rate': 4.154727793696275e-06, 'epoch': 45.96}\n",
      "{'loss': 0.1626, 'learning_rate': 2.7220630372492837e-06, 'epoch': 47.35}\n",
      "{'loss': 0.1523, 'learning_rate': 1.2893982808022925e-06, 'epoch': 48.75}\n",
      "{'train_runtime': 6928.0496, 'train_samples_per_second': 20.698, 'train_steps_per_second': 2.591, 'train_loss': 0.1643956367683942, 'epoch': 50.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca968bc0c3e4bb2ac3ab7878d1e80db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.122591033577919, 'eval_accuracy': 0.4895397489539749, 'eval_runtime': 26.635, 'eval_samples_per_second': 26.919, 'eval_steps_per_second': 3.379, 'epoch': 50.0}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the accuracy\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert_sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9af70b37b14036a5624cc96ae2b22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1596, 'learning_rate': 5e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1644, 'learning_rate': 4.856733524355301e-05, 'epoch': 2.79}\n",
      "{'loss': 0.1641, 'learning_rate': 4.713467048710602e-05, 'epoch': 4.18}\n",
      "{'loss': 0.1612, 'learning_rate': 4.570200573065903e-05, 'epoch': 5.57}\n",
      "{'loss': 0.1571, 'learning_rate': 4.426934097421204e-05, 'epoch': 6.96}\n",
      "{'loss': 0.1664, 'learning_rate': 4.2836676217765046e-05, 'epoch': 8.36}\n",
      "{'loss': 0.1603, 'learning_rate': 4.140401146131805e-05, 'epoch': 9.75}\n",
      "{'loss': 0.1603, 'learning_rate': 3.997134670487106e-05, 'epoch': 11.14}\n",
      "{'loss': 0.1667, 'learning_rate': 3.853868194842407e-05, 'epoch': 12.53}\n",
      "{'loss': 0.1623, 'learning_rate': 3.7106017191977077e-05, 'epoch': 13.93}\n",
      "{'loss': 0.1464, 'learning_rate': 3.567335243553009e-05, 'epoch': 15.32}\n",
      "{'loss': 0.17, 'learning_rate': 3.4240687679083095e-05, 'epoch': 16.71}\n",
      "{'loss': 0.1577, 'learning_rate': 3.280802292263611e-05, 'epoch': 18.11}\n",
      "{'loss': 0.1622, 'learning_rate': 3.1375358166189114e-05, 'epoch': 19.5}\n",
      "{'loss': 0.1594, 'learning_rate': 2.9942693409742123e-05, 'epoch': 20.89}\n",
      "{'loss': 0.1637, 'learning_rate': 2.851002865329513e-05, 'epoch': 22.28}\n",
      "{'loss': 0.1545, 'learning_rate': 2.707736389684814e-05, 'epoch': 23.68}\n",
      "{'loss': 0.1627, 'learning_rate': 2.5644699140401145e-05, 'epoch': 25.07}\n",
      "{'loss': 0.1597, 'learning_rate': 2.4212034383954154e-05, 'epoch': 26.46}\n",
      "{'loss': 0.1616, 'learning_rate': 2.2779369627507164e-05, 'epoch': 27.86}\n",
      "{'loss': 0.1646, 'learning_rate': 2.1346704871060173e-05, 'epoch': 29.25}\n",
      "{'loss': 0.16, 'learning_rate': 1.991404011461318e-05, 'epoch': 30.64}\n",
      "{'loss': 0.1568, 'learning_rate': 1.8481375358166188e-05, 'epoch': 32.03}\n",
      "{'loss': 0.1611, 'learning_rate': 1.7048710601719198e-05, 'epoch': 33.43}\n",
      "{'loss': 0.1633, 'learning_rate': 1.5616045845272207e-05, 'epoch': 34.82}\n",
      "{'loss': 0.1584, 'learning_rate': 1.4183381088825215e-05, 'epoch': 36.21}\n",
      "{'loss': 0.1611, 'learning_rate': 1.2750716332378224e-05, 'epoch': 37.6}\n",
      "{'loss': 0.1587, 'learning_rate': 1.1318051575931232e-05, 'epoch': 39.0}\n",
      "{'loss': 0.1507, 'learning_rate': 9.885386819484241e-06, 'epoch': 40.39}\n",
      "{'loss': 0.1705, 'learning_rate': 8.452722063037249e-06, 'epoch': 41.78}\n",
      "{'loss': 0.1536, 'learning_rate': 7.020057306590258e-06, 'epoch': 43.18}\n",
      "{'loss': 0.1607, 'learning_rate': 5.587392550143267e-06, 'epoch': 44.57}\n",
      "{'loss': 0.1615, 'learning_rate': 4.154727793696275e-06, 'epoch': 45.96}\n",
      "{'loss': 0.1659, 'learning_rate': 2.7220630372492837e-06, 'epoch': 47.35}\n",
      "{'loss': 0.1574, 'learning_rate': 1.2893982808022925e-06, 'epoch': 48.75}\n",
      "{'train_runtime': 6853.875, 'train_samples_per_second': 20.922, 'train_steps_per_second': 2.619, 'train_loss': 0.1605433285003917, 'epoch': 50.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59f4c47c6ea43ff9274a738f057d759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12130001187324524, 'eval_accuracy': 0.4895397489539749, 'eval_runtime': 26.3239, 'eval_samples_per_second': 27.238, 'eval_steps_per_second': 3.419, 'epoch': 50.0}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the accuracy\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
