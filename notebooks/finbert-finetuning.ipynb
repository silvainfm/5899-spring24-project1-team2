{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import BertTokenizer, BertModel,BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = pd.read_csv('/Users/stoneman/Library/CloudStorage/OneDrive-Vanderbilt/AB_Project/5899-spring24-project1-team2/sentiment_updated_sortedDate_Training.csv', header=0)\n",
    "data_prediction = pd.read_csv('/Users/stoneman/Library/CloudStorage/OneDrive-Vanderbilt/AB_Project/5899-spring24-project1-team2/sentiment_updated_sortedDate_Testing.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>adjusted_date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Osaka Governor Hirofumi Yoshimura said that th...</td>\n",
       "      <td>Years of delay to plans for Japan‚Äö√Ñ√¥s firs...</td>\n",
       "      <td>MGM</td>\n",
       "      <td>9/15/2022 0:00</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MetLife (MET) is a Finance stock that has seen...</td>\n",
       "      <td>Dividends are one of the best benefits to bein...</td>\n",
       "      <td>MET</td>\n",
       "      <td>9/15/2022 0:00</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  Osaka Governor Hirofumi Yoshimura said that th...   \n",
       "1  MetLife (MET) is a Finance stock that has seen...   \n",
       "\n",
       "                                         description Ticker   adjusted_date  \\\n",
       "0  Years of delay to plans for Japan‚Äö√Ñ√¥s firs...    MGM  9/15/2022 0:00   \n",
       "1  Dividends are one of the best benefits to bein...    MET  9/15/2022 0:00   \n",
       "\n",
       "  sentiment  \n",
       "0  negative  \n",
       "1  positive  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_training count    1086.000000\n",
      "mean      115.712707\n",
      "std        45.145769\n",
      "min        39.000000\n",
      "25%        90.000000\n",
      "50%       111.000000\n",
      "75%       132.000000\n",
      "max       817.000000\n",
      "Name: summary, dtype: float64\n",
      "data_prediction count    3268.000000\n",
      "mean      120.356487\n",
      "std        40.700251\n",
      "min        39.000000\n",
      "25%        94.000000\n",
      "50%       117.000000\n",
      "75%       141.000000\n",
      "max       540.000000\n",
      "Name: summary, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the average length of the columns: summary and description\n",
    "print('data_training', data_training['summary'].apply(lambda x: len(x.split())).describe())\n",
    "print('data_prediction', data_prediction['summary'].apply(lambda x: len(x.split())).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    809\n",
       "negative    251\n",
       "neutral      26\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training['labels'] = data_training['sentiment'].map({'positive': 0, 'neutral': 2, 'negative': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>adjusted_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Osaka Governor Hirofumi Yoshimura said that th...</td>\n",
       "      <td>Years of delay to plans for Japan‚Äö√Ñ√¥s firs...</td>\n",
       "      <td>MGM</td>\n",
       "      <td>9/15/2022 0:00</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MetLife (MET) is a Finance stock that has seen...</td>\n",
       "      <td>Dividends are one of the best benefits to bein...</td>\n",
       "      <td>MET</td>\n",
       "      <td>9/15/2022 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  Osaka Governor Hirofumi Yoshimura said that th...   \n",
       "1  MetLife (MET) is a Finance stock that has seen...   \n",
       "\n",
       "                                         description Ticker   adjusted_date  \\\n",
       "0  Years of delay to plans for Japan‚Äö√Ñ√¥s firs...    MGM  9/15/2022 0:00   \n",
       "1  Dividends are one of the best benefits to bein...    MET  9/15/2022 0:00   \n",
       "\n",
       "  sentiment  labels  \n",
       "0  negative       1  \n",
       "1  positive       0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = data_training[['summary', 'labels']]\n",
    "data_training.columns = ['text', 'labels']\n",
    "data_training.to_csv('data_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Osaka Governor Hirofumi Yoshimura said that th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MetLife (MET) is a Finance stock that has seen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  Osaka Governor Hirofumi Yoshimura said that th...       1\n",
       "1  MetLife (MET) is a Finance stock that has seen...       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_training[['text', 'labels']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 1086\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 868\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 218\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Active With The Activists: Sysco Strike Ends; Teamsters Declare Victory. We look forward to getting back to business as usual and returning our focus to servicing our customers and community.‚Äö√Ñ√π\\n\\nThe New England Sysco truck drivers‚Äö√Ñ√¥ union is relatively young, organized with the Teamsters several years ago due to dissatisfaction over issues including health insurance and wages. \\n\\nWith support from Teamsters Local 633 of New Hampshire, Sysco drivers picketed in Manchester, Bow, and Epping. The union failed to win a shift from the company-sponsored 401-K retirement plan to the union-sponsored pension fund.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT Model only on classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "finbert_cls_model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\")  # mps is for Apple Silicon GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU\n",
    "finbert_cls_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
       "  \"architectures\": [\n",
       "    \"BertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"positive\",\n",
       "    \"1\": \"negative\",\n",
       "    \"2\": \"neutral\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"negative\": 1,\n",
       "    \"neutral\": 2,\n",
       "    \"positive\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.27.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finbert_cls_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input = bert_tokenizer(dataset['train']['text'][2], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3341,  7925, 26566,  1733,  1132, 24498,  1158,  1146,  7352,\n",
       "          1113,  1207,  2058,  1496,  1106,  1126, 12104, 16907,  1104,  2331,\n",
       "          2205,  4481,  1496,  1106,  1344, 16935,  5600,   119,  1109, 25630,\n",
       "          1138,  5605,  1313,  7925, 26566,  1733,   112, 18155,   117,  5416,\n",
       "          1147, 17901,  1177, 10832,   117,  1105,  1103,   156,   111,   153,\n",
       "          3291, 24729, 13068, 10204,  3341, 12851, 12859, 10146,  1146,  3746,\n",
       "           119,  3078,   110,  1177,  1677,  1142,  1214,   119,  1109,  1954,\n",
       "          1903,  2603,  1104,  1807,   128,   110,  1113,  1103,  1927,  1476,\n",
       "           118,  1214,  4275, 16935,  1110,  1543,  1146, 20407,  4481,  1750,\n",
       "          8394,   117, 16863,  1103,  4528,  1104,  3685,  4481,   117,  1134,\n",
       "          1132,  3417,  1167, 19017,  1190,  1207,  2058,   119,  3341,  7925,\n",
       "         26566,  1733,  1113,  1903,  1138,  2120,  7352,  1107,  1164,  1160,\n",
       "           118, 12704,  1104,  1147,  3611,   117,  1105,  1142,  2502,  1112,\n",
       "          1103, 25112,  7275,  1206,  3685,  1105,  1207,  4481,  1144,  7604,\n",
       "           117,  1378,   170,  3945, 13542,  1107,  1103,  1231, 11794,  1162,\n",
       "          2319,   119,  7043,  1111,  1748,  3945, 25401,  1116,  1105,  4946,\n",
       "          2058, 13874,  1209,  2760,  1106,  6012,  1103,  5022, 18717,  1111,\n",
       "          1313,  7925, 26566,  1733,  1107,  1103,  1248,  1544,  1104,  1103,\n",
       "          1214,   117,  1133, 22018, 17163,  1115,  1142,  1209,  2049,  1172,\n",
       "          1106,  3118,  7246,  1106,  2319,  3404,  1105, 17024,  1621,  5793,\n",
       "          1165,  1122,  2502,  1106,  1103,  2178,  1104,  3945, 25401,  1116,\n",
       "          7042,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input = {k: v.to(device) for k, v in bert_input.items()}  # Move the input tensors to MPS device\n",
    "\n",
    "output = finbert_cls_model(**bert_input)  # Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits\n",
    "\n",
    "# Assuming `output` is the model output and `finbert_cls_model.config.id2label` is the id-to-label mapping\n",
    "probabilities = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "\n",
    "# Assuming 'probabilities' is your probabilities tensor\n",
    "probabilities_cpu = probabilities.to('cpu')\n",
    "predicted_class_index = torch.argmax(probabilities_cpu, dim=-1)\n",
    "predicted_class_index\n",
    "\n",
    "\n",
    "# Get the predicted label from the id2label mapping in the model's configuration\n",
    "predicted_label = finbert_cls_model.config.id2label[predicted_class_index.item()]\n",
    "\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/wt3y86vj50sg2s6pkc5y62l00000gn/T/ipykernel_47745/3702980183.py:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return bert_tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "# Tokenize the text data in the train and test datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the compute_metrics function for accuracy\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=finbert_cls_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109,484,547 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in finbert_cls_model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the BERT model\n",
    "for param in finbert_cls_model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,307 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in finbert_cls_model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 500/1090 [31:51<36:55,  3.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3428, 'learning_rate': 5e-05, 'epoch': 4.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1000/1090 [1:03:32<05:41,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6466, 'learning_rate': 7.627118644067798e-06, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1090/1090 [1:09:28<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4168.7992, 'train_samples_per_second': 2.082, 'train_steps_per_second': 0.261, 'train_loss': 0.968447148034332, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:37<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6671083569526672, 'eval_accuracy': 0.7201834862385321, 'eval_runtime': 101.772, 'eval_samples_per_second': 2.142, 'eval_steps_per_second': 0.275, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the accuracy\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT Model on only bert.pooler and classifier layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the BERT model\n",
    "for param in finbert_cls_model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in finbert_cls_model.bert.pooler.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592,899 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in finbert_cls_model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 500/1090 [32:34<39:15,  3.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6478, 'learning_rate': 5e-05, 'epoch': 4.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1000/1090 [1:05:39<05:58,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6325, 'learning_rate': 7.627118644067798e-06, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1090/1090 [1:11:38<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4298.2406, 'train_samples_per_second': 2.019, 'train_steps_per_second': 0.254, 'train_loss': 0.6399818245424043, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:38<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6771977543830872, 'eval_accuracy': 0.7201834862385321, 'eval_runtime': 101.9094, 'eval_samples_per_second': 2.139, 'eval_steps_per_second': 0.275, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the accuracy\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finbert_cls_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "fine_tune_model_path = \"/Users/stoneman/Library/CloudStorage/OneDrive-Vanderbilt/AB_Project/5899-spring24-project1-team2/finbert_cls_model\"\n",
    "finbert_cls_model = BertForSequenceClassification.from_pretrained(fine_tune_model_path)\n",
    "\n",
    "device = torch.device(\"mps\")  # mps is for Apple Silicon GPU\n",
    "finbert_cls_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(data, model, tokenizer, batch_size=32):\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        inputs = tokenizer(list(batch), padding=True, truncation=True, return_tensors='pt')\n",
    "        inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        batch_probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        batch_probabilities = batch_probabilities.cpu().numpy()\n",
    "        predictions.extend(batch_probabilities)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Perform batch inference\n",
    "batch_size = 32  # Adjust based on your device's memory\n",
    "probabilities = batch_predict(data_prediction['summary'], finbert_cls_model, bert_tokenizer, batch_size=batch_size)\n",
    "\n",
    "# Convert probabilities to DataFrame and concatenate with the original DataFrame\n",
    "prob_df = pd.DataFrame(probabilities, columns=['positive', 'negative', 'neutral'])\n",
    "data_prediction = pd.concat([data_prediction.reset_index(drop=True), prob_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prediction.to_csv('data_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>adjusted_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walgreens Boots Alliance has sold its remainin...</td>\n",
       "      <td>Walgreens Boots Alliance has sold its remainin...</td>\n",
       "      <td>CVS</td>\n",
       "      <td>1/3/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.786601</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.021241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nAlthough the financial companies don't mak...</td>\n",
       "      <td>In this article, we will take a look at 10 of ...</td>\n",
       "      <td>BLK</td>\n",
       "      <td>1/3/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.802762</td>\n",
       "      <td>0.173660</td>\n",
       "      <td>0.023579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cigna Group reported strong second-quarter res...</td>\n",
       "      <td>Cigna Group second-quarter results beat Wall S...</td>\n",
       "      <td>CI</td>\n",
       "      <td>1/3/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.768271</td>\n",
       "      <td>0.209153</td>\n",
       "      <td>0.022576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Environmental Protection Agency has ordere...</td>\n",
       "      <td>The Environmental Protection Agency has ordere...</td>\n",
       "      <td>NSC</td>\n",
       "      <td>1/3/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.803227</td>\n",
       "      <td>0.173373</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sen. Joe Manchin (D-WV) is blaming Secretary o...</td>\n",
       "      <td>WASHINGTON ‚Äî Sen. Joe Manchin (D-WV) is blam...</td>\n",
       "      <td>NSC</td>\n",
       "      <td>1/3/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.749967</td>\n",
       "      <td>0.232096</td>\n",
       "      <td>0.017937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>Vauxhall, the owner of Vauxhall and the UK car...</td>\n",
       "      <td>Stellantis may be premature in saying Brexit d...</td>\n",
       "      <td>F</td>\n",
       "      <td>9/7/2023 0:00</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.764649</td>\n",
       "      <td>0.199559</td>\n",
       "      <td>0.035792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>The Federal Trade Commission is seeking to blo...</td>\n",
       "      <td>The biotech company is facing a patent cliff s...</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>9/7/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.750436</td>\n",
       "      <td>0.228293</td>\n",
       "      <td>0.021271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>Ford has announced it is recalling over 1.2 mi...</td>\n",
       "      <td>Just a day after Honda announced a half a mill...</td>\n",
       "      <td>F</td>\n",
       "      <td>9/7/2023 0:00</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.789926</td>\n",
       "      <td>0.192225</td>\n",
       "      <td>0.017849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>Ford CEO Jim Farley discussed the company's st...</td>\n",
       "      <td>Ford Making EVs Means Turning the Clock Back 1...</td>\n",
       "      <td>F</td>\n",
       "      <td>9/7/2023 0:00</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.763356</td>\n",
       "      <td>0.216059</td>\n",
       "      <td>0.020585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>Former CEO of Silicon Valley Bank Greg Becker ...</td>\n",
       "      <td>The former head of Silicon Valley Bank testifi...</td>\n",
       "      <td>SBNY</td>\n",
       "      <td>9/7/2023 0:00</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.770653</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.021376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3268 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                summary  \\\n",
       "0     Walgreens Boots Alliance has sold its remainin...   \n",
       "1     \\n\\nAlthough the financial companies don't mak...   \n",
       "2     Cigna Group reported strong second-quarter res...   \n",
       "3     The Environmental Protection Agency has ordere...   \n",
       "4     Sen. Joe Manchin (D-WV) is blaming Secretary o...   \n",
       "...                                                 ...   \n",
       "3263  Vauxhall, the owner of Vauxhall and the UK car...   \n",
       "3264  The Federal Trade Commission is seeking to blo...   \n",
       "3265  Ford has announced it is recalling over 1.2 mi...   \n",
       "3266  Ford CEO Jim Farley discussed the company's st...   \n",
       "3267  Former CEO of Silicon Valley Bank Greg Becker ...   \n",
       "\n",
       "                                            description Ticker  adjusted_date  \\\n",
       "0     Walgreens Boots Alliance has sold its remainin...    CVS  1/3/2023 0:00   \n",
       "1     In this article, we will take a look at 10 of ...    BLK  1/3/2023 0:00   \n",
       "2     Cigna Group second-quarter results beat Wall S...     CI  1/3/2023 0:00   \n",
       "3     The Environmental Protection Agency has ordere...    NSC  1/3/2023 0:00   \n",
       "4     WASHINGTON ‚Äî Sen. Joe Manchin (D-WV) is blam...    NSC  1/3/2023 0:00   \n",
       "...                                                 ...    ...            ...   \n",
       "3263  Stellantis may be premature in saying Brexit d...      F  9/7/2023 0:00   \n",
       "3264  The biotech company is facing a patent cliff s...   ABBV  9/7/2023 0:00   \n",
       "3265  Just a day after Honda announced a half a mill...      F  9/7/2023 0:00   \n",
       "3266  Ford Making EVs Means Turning the Clock Back 1...      F  9/7/2023 0:00   \n",
       "3267  The former head of Silicon Valley Bank testifi...   SBNY  9/7/2023 0:00   \n",
       "\n",
       "     sentiment  positive  negative   neutral  \n",
       "0     positive  0.786601  0.192157  0.021241  \n",
       "1     positive  0.802762  0.173660  0.023579  \n",
       "2     positive  0.768271  0.209153  0.022576  \n",
       "3     positive  0.803227  0.173373  0.023400  \n",
       "4     positive  0.749967  0.232096  0.017937  \n",
       "...        ...       ...       ...       ...  \n",
       "3263  negative  0.764649  0.199559  0.035792  \n",
       "3264  positive  0.750436  0.228293  0.021271  \n",
       "3265  positive  0.789926  0.192225  0.017849  \n",
       "3266  negative  0.763356  0.216059  0.020585  \n",
       "3267  negative  0.770653  0.207971  0.021376  \n",
       "\n",
       "[3268 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
