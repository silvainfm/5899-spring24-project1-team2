{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 5899 Project 1 Starter Code\n",
    "\n",
    "This notebook is intended to serve as a guide to help you develop a supervised sentiment analysis model for the news article given in the provided dataset. The default sentiments are positive, negative, and neutral, but each team may also define their own more granular scale based on those three sentiments. In addition to the data provided in the `project1_annotation_cleaned.xlsx` file, teams may also consider using daily returns or other stock data from Yahoo Finance. After teams develop the sentiment classification model, the sentiment and daily returns along with other information will be used to construct asset portfolios. The final deliverables include the following:\n",
    "- All project source code in a private GitHub repo within the DSI org https://github.com/vanderbilt-data-science - please use the prefix `5899-spring24` for your project name. \n",
    "- All final versions of the data used for the analysis and constructed portfolios\n",
    "- A README page detailing the results and performance of your portfolios\n",
    "- A team project presentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "The experimental framework defined below is based on the Stanford Sentiment Treebank ([SST](https://nlp.stanford.edu/sentiment/index.html)). You don't have to use it, but it may be helpful to you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A softmax baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(text):\n",
    "    return Counter(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin wrapper around `LogisticRegression` for the sake of `sst.experiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental run with some notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For detailed function usage, see sst.py\n",
    "softmax_experiment = sst.experiment(\n",
    "    your_train_dataframe, # TODO: add your own here\n",
    "    unigrams_phi,                \n",
    "    fit_softmax_classifier,       \n",
    "    assess_dataframes=[your_eval_datasets]) # TODO: add your own here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax_experiment` contains a lot of information that you can use for error analysis; see [this section below](#Error-analysis) for starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNClassifier wrapper\n",
    "\n",
    "This section illustrates how to use `sst.experiment` with `TorchRNNClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To featurize examples for an RNN, we can just get the words in order, letting the model take care of mapping them into an embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wrapper gets the vocabulary using `sst.get_vocab`. If you want to use pretrained word representations in here, then you can have `fit_rnn_classifier` build that space too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn_classifier(X, y):\n",
    "    sst_glove_vocab = utils.get_vocab(X, mincount=2)\n",
    "    mod = TorchRNNClassifier(\n",
    "        sst_glove_vocab,\n",
    "        early_stopping=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For detailed function usage, see sst.py\n",
    "rnn_experiment = sst.experiment(\n",
    "    your_train_dataframe, # TODO: add your own here\n",
    "    rnn_phi,\n",
    "    fit_rnn_classifier,\n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_dataframes=[your_eval_datasets]) # TODO: add your own here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "This section begins to build an error-analysis framework using the dicts returned by `sst.experiment`. These have the following structure:\n",
    "\n",
    "```\n",
    "'model': trained model\n",
    "'phi': the feature function used\n",
    "'train_dataset':\n",
    "   'X': feature matrix\n",
    "   'y': list of labels\n",
    "   'vectorizer': DictVectorizer,\n",
    "   'raw_examples': list of raw inputs, before featurizing   \n",
    "'assess_datasets': list of datasets, each with the same structure as the value of 'train_dataset'\n",
    "'predictions': list of lists of predictions on the assessment datasets\n",
    "'metric': `score_func.__name__`, where `score_func` is an `sst.experiment` argument\n",
    "'score': the `score_func` score on the each of the assessment dataasets\n",
    "```\n",
    "The following function just finds mistakes, and returns a `pd.DataFrame` for easy subsequent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(experiment):\n",
    "    \"\"\"Find mistaken predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment : dict\n",
    "        As returned by `sst.experiment`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for i, dataset in enumerate(experiment['assess_datasets']):\n",
    "        df = pd.DataFrame({\n",
    "            'raw_examples': dataset['raw_examples'],\n",
    "            'predicted': experiment['predictions'][i],\n",
    "            'gold': dataset['y']})\n",
    "        df['correct'] = df['predicted'] == df['gold']\n",
    "        df['dataset'] = i\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your implementations below\n",
    "The following code cells contain a number of functions that your team will need to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the `get_token_counts` function such that, given a `pd.DataFrame` in the format of our datasets, \n",
    "# it tokenizes the example sentences based on whitespace and creates a count distribution over all of the tokens. \n",
    "# The function should return a `pd.Series` sorted by frequency; if you create a count dictionary `d`, then \n",
    "# `pd.Series(d).sort_values(ascending=False)` will give you what you need.\n",
    "\n",
    "def get_token_counts(df):\n",
    "    \n",
    "    token_counts = ##### YOUR CODE HERE\n",
    "    return token_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_token_counts(func):\n",
    "    df = pd.DataFrame([\n",
    "        {'sentence': 'a a b'},\n",
    "        {'sentence': 'a b a'},\n",
    "        {'sentence': 'a a a b.'}])\n",
    "    result = func(df)\n",
    "    for token, expected in (('a', 7), ('b', 2), ('b.', 1)):\n",
    "        actual = result.loc[token]\n",
    "        assert actual == expected, \\\n",
    "            \"For token {}, expected {}; got {}\".format(\n",
    "            token, expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of the next few cells is to get you thinking more about this strong baseline feature representation scheme for your dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "\n",
    "\n",
    "# The following is a model wrapper function around TorchShallowNeuralClassifier. This function should implement \n",
    "# hyperparameter search according to this specification:\n",
    "\n",
    "# * Set `early_stopping=True` for all experiments.\n",
    "# * Using 3-fold cross-validation, exhaustively explore this set of hyperparameter combinations:\n",
    "#   * The hidden dimensionality at 50, 100, and 200.\n",
    "#   * The hidden activation function as `nn.Tanh()` and `nn.ReLU()`.\n",
    "# * For all other parameters to `TorchShallowNeuralClassifier`, use the defaults.\n",
    "\n",
    "def fit_shallow_neural_classifier_with_hyperparameter_search(X, y):\n",
    "    \n",
    "    ##### YOUR CODE HERE\n",
    "    base_model = # TorchShallowNeuralClassifier(##### YOUR CODE HERE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    opt_model = # utils.fit_classifier_with_hyperparameter_search(##### YOUR CODE HERE)\n",
    "    return opt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the use of BERT as encoder for text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import vsm\n",
    "\n",
    "\n",
    "# Instantiate a Bert model and tokenizer based on `bert_weights_name`:\n",
    "bert_weights_name = 'bert-base-uncased'\n",
    "##### YOUR CODE HERE \n",
    "\n",
    "# setting up a tokenizer and a model with BERT\n",
    "\n",
    "\n",
    "# implement the function `hf_cls_phi` that uses Hugging Face functionality to encode individual examples with BERT \n",
    "# and returns the final output representation above the [CLS] token.\n",
    "\n",
    "def hf_cls_phi(text):\n",
    "    # Get the ids. `vsm.hf_encode` will help; be sure to\n",
    "    # set `add_special_tokens=True`.\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # Get the BERT representations. `vsm.hf_represent` will help:\n",
    "    ##### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # Index into `reps` to get the representation above [CLS].\n",
    "    # The shape of `reps` should be (1, n, 768), where n is the\n",
    "    # number of tokens. You need the 0th element of the 2nd dim:\n",
    "    ##### YOUR CODE HERE\n",
    "    # cls_rep = \n",
    "\n",
    "    # These conversions should ensure that you can work with the\n",
    "    # representations flexibly. \n",
    "    return cls_rep.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hf_cls_phi(func):\n",
    "    rep = func(\"Just testing!\")\n",
    "\n",
    "    expected_shape = (768,)\n",
    "    result_shape = rep.shape\n",
    "    assert rep.shape == (768,), \\\n",
    "        \"Expected shape {}; got {}\".format(\n",
    "        expected_shape, result_shape)\n",
    "\n",
    "    # String conversion to avoid precision errors:\n",
    "    expected_first_val = str(0.1709)\n",
    "    result_first_val = \"{0:.04f}\".format(rep[0])\n",
    "\n",
    "    assert expected_first_val == result_first_val, \\\n",
    "        (\"Unexpected representation values. Expected the \"\n",
    "        \"first value to be {}; got {}\".format(\n",
    "            expected_first_val, result_first_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions of `predict_one` guide you to map a text (str) directly to a label prediction – one of 'positive', 'negative', 'neutral'. You may need to modify the code below to work for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_softmax(text):\n",
    "    # Singleton list of feature dicts:\n",
    "    feats = [softmax_experiment['phi'](text)]\n",
    "    # Vectorize to get a feature matrix:\n",
    "    X = softmax_experiment['train_dataset']['vectorizer'].transform(feats)\n",
    "    # Standard sklearn `predict` step:\n",
    "    preds = softmax_experiment['model'].predict(X)\n",
    "    # Be sure to return the only member of the predictions,\n",
    "    # rather than the singleton list:\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used an RNN like the one we demoed above, then featurization is a bit more straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_rnn(text):\n",
    "    # List of tokenized examples:\n",
    "    X = [rnn_experiment['phi'](text)]\n",
    "    # Standard `predict` step on a list of lists of str:\n",
    "    preds = rnn_experiment['model'].predict(X)\n",
    "    # Be sure to return the only member of the predictions,\n",
    "    # rather than the singleton list:\n",
    "    return preds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
